{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WGAN-GP: Video Frame Generation using Wasserstein GAN with Gradient Penalty\n",
    "\n",
    "**Author:** Fernando Campa and Gabriel Vanderklok \n",
    "**Course:** CST-435 Deep Learning  \n",
    "**Date:** 12/11/2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Problem Statement\n\nBuild a **Wasserstein GAN with Gradient Penalty (WGAN-GP)** to generate realistic video frame images at 200x200 resolution. Extract every 10th frame from input video for training.\n\n**WGAN-GP advantages over traditional GANs:**\n- More stable training dynamics\n- Meaningful loss metrics\n- Addresses mode collapse and vanishing gradients"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Algorithm\n\n### 2.1 Wasserstein GAN Theory\n\nWGAN uses **Wasserstein distance** (Earth Mover's Distance) instead of Binary Cross-Entropy.\n\n**Traditional GAN Loss:**\n$$\\min_G \\max_D \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n\n**WGAN Loss:**\n$$\\min_G \\max_{D \\in \\mathcal{D}} \\mathbb{E}_{x \\sim p_{data}}[D(x)] - \\mathbb{E}_{z \\sim p_z}[D(G(z))]$$\n\nWhere $\\mathcal{D}$ is the set of 1-Lipschitz functions.\n\n### 2.2 Gradient Penalty\n\nEnforce Lipschitz constraint with **gradient penalty**:\n\n$$\\mathcal{L}_{GP} = \\lambda \\mathbb{E}_{\\hat{x} \\sim p_{\\hat{x}}}[(||\\nabla_{\\hat{x}} D(\\hat{x})||_2 - 1)^2]$$\n\nWhere $\\hat{x} = \\alpha x_{real} + (1 - \\alpha) x_{fake}, \\quad \\alpha \\sim U[0,1]$\n\n### 2.3 Training Algorithm\n\n```\nFor each epoch:\n    For each batch:\n        # Train Critic (5 iterations)\n        For k = 1 to 5:\n            1. Sample real images\n            2. Generate fake images\n            3. Compute Wasserstein loss\n            4. Compute gradient penalty\n            5. Update critic\n        \n        # Train Generator (1 iteration)\n        1. Generate fake images\n        2. Compute loss\n        3. Update generator\n```\n\n### 2.4 Hyperparameters\n\n| Parameter | Value | Rationale |\n|-----------|-------|----------|\n| Œª (GP) | 10 | WGAN-GP paper standard |\n| Critic iterations | 5 | Stable gradients |\n| Learning rate | 0.0001 | Stability |\n| Adam Œ≤1, Œ≤2 | 0.0, 0.9 | WGAN recommended |\n| Latent dimension | 128 | Image complexity |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation\n",
    "\n",
    "### 3.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class to store all hyperparameters\n",
    "class Config:\n",
    "    \"\"\"Hyperparameters and settings for WGAN-GP training.\"\"\"\n",
    "    \n",
    "    # Image settings\n",
    "    IMAGE_SIZE = 200          # Output image resolution (200x200)\n",
    "    CHANNELS = 3              # RGB images\n",
    "    \n",
    "    # Model architecture\n",
    "    LATENT_DIM = 128          # Size of random noise vector (z)\n",
    "    GEN_FEATURES = 64         # Base feature maps in generator\n",
    "    CRITIC_FEATURES = 64      # Base feature maps in critic\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 16           # Images per batch\n",
    "    NUM_EPOCHS = 500          # Total training epochs\n",
    "    LEARNING_RATE = 0.0001    # Adam learning rate\n",
    "    BETA1 = 0.0               # Adam beta1 (momentum)\n",
    "    BETA2 = 0.9               # Adam beta2\n",
    "    \n",
    "    # WGAN-GP specific\n",
    "    CRITIC_ITERATIONS = 5     # Critic updates per generator update\n",
    "    LAMBDA_GP = 10            # Gradient penalty coefficient\n",
    "    \n",
    "    # Data settings\n",
    "    FRAME_INTERVAL = 10       # Extract every Nth frame from video\n",
    "    \n",
    "    # Paths\n",
    "    VIDEO_PATH = \"front.mp4\"\n",
    "    OUTPUT_DIR = \"output\"\n",
    "    FRAMES_DIR = \"output/frames\"\n",
    "    SAMPLES_DIR = \"output/samples\"\n",
    "    CHECKPOINTS_DIR = \"output/checkpoints\"\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(config.FRAMES_DIR, exist_ok=True)\n",
    "os.makedirs(config.SAMPLES_DIR, exist_ok=True)\n",
    "os.makedirs(config.CHECKPOINTS_DIR, exist_ok=True)\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 Frame Extraction\n\nExtract every 10th frame from video to reduce redundancy while maintaining visual diversity."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path: str, output_dir: str, interval: int = 10) -> int:\n",
    "    \"\"\"\n",
    "    Extract every nth frame from a video file.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video file\n",
    "        output_dir: Directory to save extracted frames\n",
    "        interval: Extract every nth frame (default: 10)\n",
    "    \n",
    "    Returns:\n",
    "        Number of frames extracted\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    print(f\"Video Properties:\")\n",
    "    print(f\"  - Total frames: {total_frames}\")\n",
    "    print(f\"  - FPS: {fps:.2f}\")\n",
    "    print(f\"  - Resolution: {width}x{height}\")\n",
    "    print(f\"  - Duration: {total_frames/fps:.2f} seconds\")\n",
    "    print(f\"\\nExtracting every {interval}th frame...\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    saved_count = 0\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(total=total_frames // interval, desc=\"Extracting frames\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % interval == 0:\n",
    "            # Convert BGR (OpenCV) to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(frame_rgb)\n",
    "            \n",
    "            # Save frame as PNG\n",
    "            save_path = os.path.join(output_dir, f\"frame_{saved_count:06d}.png\")\n",
    "            img.save(save_path)\n",
    "            saved_count += 1\n",
    "            pbar.update(1)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    pbar.close()\n",
    "    \n",
    "    print(f\"\\nExtracted {saved_count} frames to {output_dir}\")\n",
    "    return saved_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frames from video\n",
    "num_frames = extract_frames(config.VIDEO_PATH, config.FRAMES_DIR, config.FRAME_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.4 Dataset and DataLoader"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading extracted video frames.\n",
    "    \n",
    "    Applies transformations:\n",
    "    - Resize to target dimensions\n",
    "    - Convert to tensor\n",
    "    - Normalize to [-1, 1] range (required for Tanh output)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, frames_dir: str, image_size: int = 200):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frames_dir: Directory containing frame images\n",
    "            image_size: Target size for resizing (default: 200)\n",
    "        \"\"\"\n",
    "        self.frames_dir = frames_dir\n",
    "        \n",
    "        # Get list of image files\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(frames_dir) \n",
    "            if f.endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ])\n",
    "        \n",
    "        if len(self.image_files) == 0:\n",
    "            raise ValueError(f\"No images found in {frames_dir}\")\n",
    "        \n",
    "        # Define image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),  # Resize to 200x200\n",
    "            transforms.ToTensor(),                         # Convert to tensor [0, 1]\n",
    "            transforms.Normalize([0.5, 0.5, 0.5],          # Normalize to [-1, 1]\n",
    "                                 [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return total number of images.\"\"\"\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and transform a single image.\n",
    "        \n",
    "        Args:\n",
    "            idx: Index of image to load\n",
    "        \n",
    "        Returns:\n",
    "            Transformed image tensor\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.frames_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        return self.transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = FrameDataset(config.FRAMES_DIR, config.IMAGE_SIZE)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,              # Shuffle for training\n",
    "    num_workers=0,             # Set to 0 for Windows compatibility\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} images\")\n",
    "print(f\"Batches per epoch: {len(dataloader)}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.5 Visualize Training Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, title=\"Images\", nrow=4):\n",
    "    \"\"\"\n",
    "    Display a grid of images.\n",
    "    \n",
    "    Args:\n",
    "        images: Tensor of images (N, C, H, W) in range [-1, 1]\n",
    "        title: Plot title\n",
    "        nrow: Number of images per row\n",
    "    \"\"\"\n",
    "    # Denormalize from [-1, 1] to [0, 1]\n",
    "    images = (images + 1) / 2\n",
    "    images = images.clamp(0, 1)\n",
    "    \n",
    "    # Create grid\n",
    "    grid = make_grid(images, nrow=nrow, padding=2)\n",
    "    \n",
    "    # Convert to numpy for matplotlib\n",
    "    grid_np = grid.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(grid_np)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get a batch of real images\n",
    "real_batch = next(iter(dataloader))\n",
    "show_images(real_batch[:16], \"Sample Training Images (Real Frames)\", nrow=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Neural Network Architecture\n\n### 4.1 Generator Network\n\nTransforms 128-dimensional latent vector to 200x200 RGB image via progressive upsampling.\n\n**Architecture:** 5√ó5 ‚Üí 10√ó10 ‚Üí 25√ó25 ‚Üí 50√ó50 ‚Üí 100√ó100 ‚Üí 200√ó200\n- Batch Normalization for stability\n- LeakyReLU activation\n- Tanh output ([-1, 1] range)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator Network for WGAN-GP.\n",
    "    \n",
    "    Transforms a random latent vector into a 200x200 RGB image.\n",
    "    Uses transposed convolutions and upsampling for progressive\n",
    "    spatial resolution increase.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim: int, channels: int, features: int):\n",
    "        \"\"\"\n",
    "        Initialize Generator network.\n",
    "        \n",
    "        Args:\n",
    "            latent_dim: Size of input noise vector (default: 128)\n",
    "            channels: Number of output channels (3 for RGB)\n",
    "            features: Base number of feature maps (default: 64)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.init_size = 5  # Initial spatial size before upsampling\n",
    "        \n",
    "        # Input layer: Transform latent vector to feature maps\n",
    "        # Output shape: (batch, features*16, 5, 5)\n",
    "        self.l1 = nn.Sequential(\n",
    "            nn.Linear(latent_dim, features * 16 * self.init_size * self.init_size)\n",
    "        )\n",
    "        \n",
    "        # Convolutional blocks with upsampling\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Block 1: 5x5 -> 10x10\n",
    "            nn.BatchNorm2d(features * 16),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(features * 16, features * 8, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(features * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 2: 10x10 -> 25x25\n",
    "            nn.Upsample(size=(25, 25)),\n",
    "            nn.Conv2d(features * 8, features * 4, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(features * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 3: 25x25 -> 50x50\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(features * 4, features * 2, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 4: 50x50 -> 100x100\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(features * 2, features, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(features),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Block 5: 100x100 -> 200x200 (Output layer)\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(features, channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh()  # Output range: [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass through generator.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent vector tensor of shape (batch_size, latent_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Generated images of shape (batch_size, 3, 200, 200)\n",
    "        \"\"\"\n",
    "        # Transform latent vector to initial feature maps\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], -1, self.init_size, self.init_size)\n",
    "        \n",
    "        # Apply convolutional blocks\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 4.2 Critic (Discriminator) Network\n\nEvaluates images and outputs scalar score (higher = more real).\n\n**Architecture:** Progressive downsampling 200 ‚Üí 100 ‚Üí 50 ‚Üí 25 ‚Üí 12 ‚Üí 6\n- Instance Normalization (better than BatchNorm for WGAN-GP)\n- LeakyReLU activation\n- Linear output (no sigmoid)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic (Discriminator) Network for WGAN-GP.\n",
    "    \n",
    "    Takes an image and outputs a scalar score indicating\n",
    "    how \"real\" the image appears. Higher scores = more real.\n",
    "    \n",
    "    Note: Called \"Critic\" in WGAN terminology because it doesn't\n",
    "    output a probability (no sigmoid activation).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, features: int):\n",
    "        \"\"\"\n",
    "        Initialize Critic network.\n",
    "        \n",
    "        Args:\n",
    "            channels: Number of input channels (3 for RGB)\n",
    "            features: Base number of feature maps (default: 64)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers with strided convolutions for downsampling\n",
    "        self.model = nn.Sequential(\n",
    "            # Layer 1: 200x200 -> 100x100\n",
    "            nn.Conv2d(channels, features, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 2: 100x100 -> 50x50\n",
    "            nn.Conv2d(features, features * 2, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 3: 50x50 -> 25x25\n",
    "            nn.Conv2d(features * 2, features * 4, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 4: 25x25 -> 12x12\n",
    "            nn.Conv2d(features * 4, features * 8, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 5: 12x12 -> 6x6\n",
    "            nn.Conv2d(features * 8, features * 16, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(features * 16, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Output layer: Flatten and produce single scalar\n",
    "        # Input size: features*16 * 6 * 6\n",
    "        self.fc = nn.Linear(features * 16 * 6 * 6, 1)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        Forward pass through critic.\n",
    "        \n",
    "        Args:\n",
    "            img: Image tensor of shape (batch_size, 3, 200, 200)\n",
    "        \n",
    "        Returns:\n",
    "            Critic scores of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)  # Flatten\n",
    "        validity = self.fc(out)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Initialize Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Generator and Critic\n",
    "generator = Generator(\n",
    "    latent_dim=config.LATENT_DIM,\n",
    "    channels=config.CHANNELS,\n",
    "    features=config.GEN_FEATURES\n",
    ").to(device)\n",
    "\n",
    "critic = Critic(\n",
    "    channels=config.CHANNELS,\n",
    "    features=config.CRITIC_FEATURES\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "gen_params = sum(p.numel() for p in generator.parameters())\n",
    "critic_params = sum(p.numel() for p in critic.parameters())\n",
    "\n",
    "print(\"Generator Architecture:\")\n",
    "print(f\"  - Parameters: {gen_params:,}\")\n",
    "print(f\"  - Input: {config.LATENT_DIM}-dimensional latent vector\")\n",
    "print(f\"  - Output: {config.CHANNELS}x{config.IMAGE_SIZE}x{config.IMAGE_SIZE} image\")\n",
    "\n",
    "print(f\"\\nCritic Architecture:\")\n",
    "print(f\"  - Parameters: {critic_params:,}\")\n",
    "print(f\"  - Input: {config.CHANNELS}x{config.IMAGE_SIZE}x{config.IMAGE_SIZE} image\")\n",
    "print(f\"  - Output: Scalar score\")\n",
    "\n",
    "print(f\"\\nTotal parameters: {gen_params + critic_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Training\n\n### 5.1 Gradient Penalty Function\n\nEnforces Lipschitz constraint by penalizing when gradient norm deviates from 1."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(critic, real_samples, fake_samples, device):\n",
    "    \"\"\"\n",
    "    Compute gradient penalty for WGAN-GP.\n",
    "    \n",
    "    The gradient penalty encourages the critic to have gradients\n",
    "    with norm close to 1, which enforces the Lipschitz constraint.\n",
    "    \n",
    "    Args:\n",
    "        critic: Critic network\n",
    "        real_samples: Batch of real images\n",
    "        fake_samples: Batch of generated images\n",
    "        device: Computing device (CPU/GPU)\n",
    "    \n",
    "    Returns:\n",
    "        Gradient penalty scalar\n",
    "    \"\"\"\n",
    "    batch_size = real_samples.size(0)\n",
    "    \n",
    "    # Random interpolation coefficient (uniform between 0 and 1)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "    \n",
    "    # Create interpolated samples between real and fake\n",
    "    interpolated = alpha * real_samples + (1 - alpha) * fake_samples\n",
    "    interpolated.requires_grad_(True)\n",
    "    \n",
    "    # Get critic scores for interpolated samples\n",
    "    d_interpolated = critic(interpolated)\n",
    "    \n",
    "    # Compute gradients w.r.t. interpolated samples\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(d_interpolated),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Compute gradient penalty: (||grad|| - 1)^2\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizers (Adam with WGAN-specific betas)\n",
    "opt_generator = optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    betas=(config.BETA1, config.BETA2)\n",
    ")\n",
    "\n",
    "opt_critic = optim.Adam(\n",
    "    critic.parameters(),\n",
    "    lr=config.LEARNING_RATE,\n",
    "    betas=(config.BETA1, config.BETA2)\n",
    ")\n",
    "\n",
    "# Fixed noise for consistent visualization during training\n",
    "fixed_noise = torch.randn(16, config.LATENT_DIM, device=device)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'g_losses': [],\n",
    "    'c_losses': [],\n",
    "    'epochs': []\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  - Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"  - Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"  - Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  - Critic iterations: {config.CRITIC_ITERATIONS}\")\n",
    "print(f\"  - Gradient penalty Œª: {config.LAMBDA_GP}\")\n",
    "print(f\"  - Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(f\"\\nStarting WGAN-GP training for {config.NUM_EPOCHS} epochs...\\n\")\n",
    "\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    epoch_g_loss = 0\n",
    "    epoch_c_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Progress bar for current epoch\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "    \n",
    "    for batch_idx, real_imgs in enumerate(pbar):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "        \n",
    "        # =====================\n",
    "        # Train Critic\n",
    "        # =====================\n",
    "        # Train critic multiple times per generator iteration\n",
    "        for _ in range(config.CRITIC_ITERATIONS):\n",
    "            opt_critic.zero_grad()\n",
    "            \n",
    "            # Generate fake images\n",
    "            z = torch.randn(batch_size, config.LATENT_DIM, device=device)\n",
    "            fake_imgs = generator(z).detach()\n",
    "            \n",
    "            # Critic scores for real and fake images\n",
    "            real_validity = critic(real_imgs)\n",
    "            fake_validity = critic(fake_imgs)\n",
    "            \n",
    "            # Compute gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(\n",
    "                critic, real_imgs, fake_imgs, device\n",
    "            )\n",
    "            \n",
    "            # Wasserstein loss + gradient penalty\n",
    "            # Critic wants: high scores for real, low scores for fake\n",
    "            c_loss = (\n",
    "                -torch.mean(real_validity) + \n",
    "                torch.mean(fake_validity) + \n",
    "                config.LAMBDA_GP * gradient_penalty\n",
    "            )\n",
    "            \n",
    "            c_loss.backward()\n",
    "            opt_critic.step()\n",
    "        \n",
    "        # =====================\n",
    "        # Train Generator\n",
    "        # =====================\n",
    "        opt_generator.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        z = torch.randn(batch_size, config.LATENT_DIM, device=device)\n",
    "        fake_imgs = generator(z)\n",
    "        \n",
    "        # Generator loss: wants critic to give high scores to fakes\n",
    "        g_loss = -torch.mean(critic(fake_imgs))\n",
    "        \n",
    "        g_loss.backward()\n",
    "        opt_generator.step()\n",
    "        \n",
    "        # Track losses\n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_c_loss += c_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'G_loss': f'{g_loss.item():.4f}',\n",
    "            'C_loss': f'{c_loss.item():.4f}'\n",
    "        })\n",
    "    \n",
    "    # Calculate average losses for epoch\n",
    "    avg_g_loss = epoch_g_loss / num_batches\n",
    "    avg_c_loss = epoch_c_loss / num_batches\n",
    "    \n",
    "    # Store history\n",
    "    history['g_losses'].append(avg_g_loss)\n",
    "    history['c_losses'].append(avg_c_loss)\n",
    "    history['epochs'].append(epoch + 1)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch [{epoch+1}/{config.NUM_EPOCHS}] \"\n",
    "          f\"G_loss: {avg_g_loss:.4f}, C_loss: {avg_c_loss:.4f}\")\n",
    "    \n",
    "    # Save sample images every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            fake_samples = generator(fixed_noise)\n",
    "            fake_samples = (fake_samples + 1) / 2  # Denormalize\n",
    "            save_image(\n",
    "                fake_samples,\n",
    "                os.path.join(config.SAMPLES_DIR, f\"epoch_{epoch+1:04d}.png\"),\n",
    "                nrow=4,\n",
    "                normalize=False\n",
    "            )\n",
    "        generator.train()\n",
    "    \n",
    "    # Save checkpoint every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'critic_state_dict': critic.state_dict(),\n",
    "            'opt_g_state_dict': opt_generator.state_dict(),\n",
    "            'opt_c_state_dict': opt_critic.state_dict(),\n",
    "            'history': history,\n",
    "        }, os.path.join(config.CHECKPOINTS_DIR, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'generator_state_dict': generator.state_dict(),\n",
    "    'critic_state_dict': critic.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'latent_dim': config.LATENT_DIM,\n",
    "        'channels': config.CHANNELS,\n",
    "        'image_size': config.IMAGE_SIZE,\n",
    "        'gen_features': config.GEN_FEATURES,\n",
    "        'critic_features': config.CRITIC_FEATURES,\n",
    "    }\n",
    "}, os.path.join(config.CHECKPOINTS_DIR, \"final_model.pt\"))\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Final model saved to: {config.CHECKPOINTS_DIR}/final_model.pt\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Analysis and Results\n\n### 6.1 Training Loss Curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Generator loss\n",
    "axes[0].plot(history['epochs'], history['g_losses'], 'b-', linewidth=1.5)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Generator Loss')\n",
    "axes[0].set_title('Generator Loss Over Training')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Critic loss\n",
    "axes[1].plot(history['epochs'], history['c_losses'], 'r-', linewidth=1.5)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Critic Loss')\n",
    "axes[1].set_title('Critic Loss Over Training')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/loss_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Generator Loss: {history['g_losses'][-1]:.4f}\")\n",
    "print(f\"Final Critic Loss: {history['c_losses'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.2 Generated Samples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new images\n",
    "generator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate 16 random images\n",
    "    z = torch.randn(16, config.LATENT_DIM, device=device)\n",
    "    generated_images = generator(z)\n",
    "\n",
    "# Display generated images\n",
    "show_images(generated_images, \"Generated Images (Final Model)\", nrow=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Training Progression\n",
    "\n",
    "Let's visualize how generated images improved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display training progression\n",
    "import glob\n",
    "\n",
    "sample_files = sorted(glob.glob(os.path.join(config.SAMPLES_DIR, \"epoch_*.png\")))\n",
    "\n",
    "if sample_files:\n",
    "    # Select samples at key epochs\n",
    "    num_samples = min(10, len(sample_files))\n",
    "    indices = np.linspace(0, len(sample_files)-1, num_samples, dtype=int)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img = Image.open(sample_files[idx])\n",
    "        epoch_num = int(Path(sample_files[idx]).stem.split('_')[1])\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'Epoch {epoch_num}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Training Progression: Generated Images Over Time', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/training_progression.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No sample images found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Real vs Generated Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare real and generated images side by side\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Get real images\n",
    "real_batch = next(iter(dataloader))[:4]\n",
    "\n",
    "# Generate fake images\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(4, config.LATENT_DIM, device=device)\n",
    "    fake_batch = generator(z).cpu()\n",
    "\n",
    "# Display real images (top row)\n",
    "for i in range(4):\n",
    "    img = (real_batch[i] + 1) / 2  # Denormalize\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title('Real')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Display generated images (bottom row)\n",
    "for i in range(4):\n",
    "    img = (fake_batch[i] + 1) / 2  # Denormalize\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    axes[1, i].imshow(img)\n",
    "    axes[1, i].set_title('Generated')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Real vs Generated Images Comparison', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/real_vs_generated.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.5 Latent Space Interpolation\n\nSmooth transitions demonstrate meaningful learned representations."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate between two random latent vectors\n",
    "def interpolate(z1, z2, steps=10):\n",
    "    \"\"\"Linear interpolation between two latent vectors.\"\"\"\n",
    "    ratios = np.linspace(0, 1, steps)\n",
    "    vectors = [(1 - r) * z1 + r * z2 for r in ratios]\n",
    "    return torch.stack(vectors)\n",
    "\n",
    "# Generate interpolation\n",
    "torch.manual_seed(42)\n",
    "z1 = torch.randn(1, config.LATENT_DIM, device=device)\n",
    "z2 = torch.randn(1, config.LATENT_DIM, device=device)\n",
    "\n",
    "z_interp = interpolate(z1.squeeze(), z2.squeeze(), steps=10).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    interp_images = generator(z_interp)\n",
    "\n",
    "# Display interpolation\n",
    "fig, axes = plt.subplots(1, 10, figsize=(20, 3))\n",
    "\n",
    "for i in range(10):\n",
    "    img = (interp_images[i] + 1) / 2\n",
    "    img = img.cpu().permute(1, 2, 0).numpy()\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    if i == 0:\n",
    "        axes[i].set_title('Start')\n",
    "    elif i == 9:\n",
    "        axes[i].set_title('End')\n",
    "\n",
    "plt.suptitle('Latent Space Interpolation', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/latent_interpolation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Summary\n",
    "\n",
    "### 7.1 Model Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "print(\"=\"*60)\n",
    "print(\"WGAN-GP TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Dataset:\")\n",
    "print(f\"   - Source: {config.VIDEO_PATH}\")\n",
    "print(f\"   - Frames extracted: {len(dataset)}\")\n",
    "print(f\"   - Frame interval: Every {config.FRAME_INTERVAL}th frame\")\n",
    "print(f\"   - Image size: {config.IMAGE_SIZE}x{config.IMAGE_SIZE}\")\n",
    "\n",
    "print(\"\\nüèóÔ∏è Architecture:\")\n",
    "print(f\"   - Generator parameters: {gen_params:,}\")\n",
    "print(f\"   - Critic parameters: {critic_params:,}\")\n",
    "print(f\"   - Total parameters: {gen_params + critic_params:,}\")\n",
    "print(f\"   - Latent dimension: {config.LATENT_DIM}\")\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"   - Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"   - Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"   - Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"   - Critic iterations: {config.CRITIC_ITERATIONS}\")\n",
    "print(f\"   - Gradient penalty Œª: {config.LAMBDA_GP}\")\n",
    "print(f\"   - Device: {device}\")\n",
    "\n",
    "print(\"\\nüìà Final Results:\")\n",
    "print(f\"   - Final Generator Loss: {history['g_losses'][-1]:.4f}\")\n",
    "print(f\"   - Final Critic Loss: {history['c_losses'][-1]:.4f}\")\n",
    "print(f\"   - Best Generator Loss: {min(history['g_losses']):.4f}\")\n",
    "\n",
    "print(\"\\nüìÅ Output Files:\")\n",
    "print(f\"   - Model: {config.CHECKPOINTS_DIR}/final_model.pt\")\n",
    "print(f\"   - Samples: {config.SAMPLES_DIR}/\")\n",
    "print(f\"   - Plots: output/loss_curves.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Conclusion\n\n### 8.1 Summary\n\nSuccessfully implemented **WGAN-GP** for video frame generation with stable training, quality image generation at 200√ó200, and meaningful latent space representations.\n\n### 8.2 Key Findings\n\n- **Wasserstein loss** provides more meaningful gradients than binary cross-entropy\n- **Gradient penalty** effectively enforces Lipschitz constraint without weight clipping\n- **Instance normalization** in critic works better than batch normalization for WGAN-GP\n- **Training critic more** (5:1 ratio) improves stability\n\n### 8.3 Future Improvements\n\n**IMPORTANT - Rocket League Training Data:**\n**üéÆ For Rocket League image generation, training on MULTIPLE CAMERA ANGLES is critical:**\n- **Ball cam** - follows the ball, dynamic perspectives\n- **Car cam** - fixed to car, diverse car orientations\n- **Different view angles** - varying heights, distances, and perspectives\n- This diversity enables the model to generate cars in any orientation/position, not just limited viewpoints\n- Current single-perspective training restricts output variety\n\n**Additional enhancements:**\n- **Progressive Growing GAN** for higher resolution outputs\n- **Self-Attention layers** for better global coherence\n- **Spectral Normalization** as alternative to gradient penalty\n- **FID (Fr√©chet Inception Distance)** for quantitative evaluation"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}